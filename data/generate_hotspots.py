#!/usr/bin/env python3
"""
ETL Script: å¾ accidents table ç”Ÿæˆ hotspots table

æ­¤è…³æœ¬æœƒï¼š
1. å¾ accidents table è®€å–æŒ‡å®šæ™‚é–“ç¯„åœçš„äº‹æ•…è³‡æ–™
2. ä½¿ç”¨ DBSCAN èšé¡æ¼”ç®—æ³•è­˜åˆ¥äº‹æ•…ç†±é»
3. è¨ˆç®—æ¯å€‹ç†±é»çš„çµ±è¨ˆè³‡è¨Šï¼ˆä¸­å¿ƒé»ã€åŠå¾‘ã€äº‹æ•…æ•¸ç­‰ï¼‰
4. å°‡çµæœå¯«å…¥ hotspots table

ä½¿ç”¨ç¯„ä¾‹ï¼š
  uv run python data/generate_hotspots.py --database-url "$DATABASE_URL"
  uv run python data/generate_hotspots.py --period-days 365 --min-accidents 5
"""

import argparse
import sys
from datetime import datetime, timedelta, date, timezone
from decimal import Decimal
from typing import List, Dict, Tuple
import uuid
import json

try:
    import psycopg2
    from psycopg2.extras import execute_values, RealDictCursor
except ImportError:
    print("âŒ è«‹å…ˆå®‰è£ psycopg2: uv add psycopg2-binary", file=sys.stderr)
    sys.exit(1)

try:
    import numpy as np
    from sklearn.cluster import DBSCAN
except ImportError:
    print("âŒ è«‹å…ˆå®‰è£ scikit-learn: uv add scikit-learn", file=sys.stderr)
    sys.exit(1)

try:
    from geopy.distance import geodesic
except ImportError:
    print("âŒ è«‹å…ˆå®‰è£ geopy: uv add geopy", file=sys.stderr)
    sys.exit(1)


def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(
        description="å¾ accidents table ç”Ÿæˆ hotspots table (DBSCAN èšé¡åˆ†æ)"
    )

    parser.add_argument(
        "--database-url",
        type=str,
        required=True,
        help="PostgreSQL é€£ç·šå­—ä¸² (ä¾‹å¦‚: postgresql://user:pass@host:5432/dbname)",
    )

    parser.add_argument(
        "--period-days",
        type=int,
        default=365,
        help="åˆ†æéå»å¹¾å¤©çš„äº‹æ•…è³‡æ–™ (é è¨­: 365)",
    )

    parser.add_argument(
        "--epsilon-meters",
        type=int,
        default=500,
        help="DBSCAN epsilon åƒæ•¸ï¼šèšé¡åŠå¾‘ï¼ˆå…¬å°ºï¼‰(é è¨­: 500)",
    )

    parser.add_argument(
        "--min-accidents",
        type=int,
        default=5,
        help="DBSCAN min_samples åƒæ•¸ï¼šæœ€å°äº‹æ•…æ•¸ (é è¨­: 5)",
    )

    parser.add_argument(
        "--clear-existing",
        action="store_true",
        help="æ¸…é™¤ç¾æœ‰çš„ hotspots è³‡æ–™",
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="æ¸¬è©¦æ¨¡å¼ï¼šä¸å¯«å…¥è³‡æ–™åº«",
    )

    return parser.parse_args()


def fetch_accidents(conn, cutoff_date: datetime) -> List[Dict]:
    """
    å¾ accidents table è®€å–æŒ‡å®šæ™‚é–“ç¯„åœçš„äº‹æ•…è³‡æ–™

    Args:
        conn: è³‡æ–™åº«é€£ç·š
        cutoff_date: æœ€æ—©äº‹æ•…æ™‚é–“ï¼ˆä¹‹å‰çš„ä¸ç´å…¥åˆ†æï¼‰

    Returns:
        äº‹æ•…è¨˜éŒ„åˆ—è¡¨
    """
    print(f"ğŸ“Š è®€å– {cutoff_date.date()} ä¹‹å¾Œçš„äº‹æ•…è³‡æ–™...")

    query = """
        SELECT 
            id::text,
            source_type,
            occurred_at,
            latitude,
            longitude
        FROM accidents
        WHERE occurred_at >= %s
        ORDER BY occurred_at DESC
    """

    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute(query, (cutoff_date,))
        accidents = cur.fetchall()

    print(f"âœ… è®€å– {len(accidents)} ç­†äº‹æ•…è¨˜éŒ„")
    return accidents


def perform_dbscan_clustering(
    accidents: List[Dict], epsilon_meters: int, min_samples: int
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨ DBSCAN æ¼”ç®—æ³•é€²è¡Œèšé¡åˆ†æ

    Args:
        accidents: äº‹æ•…è¨˜éŒ„åˆ—è¡¨
        epsilon_meters: èšé¡åŠå¾‘ï¼ˆå…¬å°ºï¼‰
        min_samples: æœ€å°äº‹æ•…æ•¸

    Returns:
        (coordinates, labels) - åº§æ¨™é™£åˆ—å’Œèšé¡æ¨™ç±¤
    """
    print(f"\nğŸ”¬ åŸ·è¡Œ DBSCAN èšé¡åˆ†æ...")
    print(f"   åƒæ•¸: epsilon={epsilon_meters}m, min_samples={min_samples}")

    # æº–å‚™åº§æ¨™è³‡æ–™
    coordinates = np.array(
        [[float(acc["latitude"]), float(acc["longitude"])] for acc in accidents]
    )

    # åŸ·è¡Œ DBSCANï¼ˆä½¿ç”¨ haversine è·é›¢è¨ˆç®—åœ°çƒè¡¨é¢è·é›¢ï¼‰
    epsilon_degrees = epsilon_meters / 111000.0  # ç´„ç•¥ï¼š1åº¦ â‰ˆ 111å…¬é‡Œ

    dbscan = DBSCAN(
        eps=epsilon_degrees,
        min_samples=min_samples,
        metric="haversine",
        algorithm="ball_tree",
    )

    labels = dbscan.fit_predict(np.radians(coordinates))

    # çµ±è¨ˆçµæœ
    unique_labels = set(labels)
    noise_points = list(labels).count(-1)
    cluster_count = len(unique_labels) - (1 if -1 in unique_labels else 0)

    print(f"âœ… èšé¡å®Œæˆï¼š")
    print(f"   - ç™¼ç¾ {cluster_count} å€‹ç†±é»")
    print(f"   - å™ªéŸ³é»ï¼ˆæœªæ­¸é¡äº‹æ•…ï¼‰: {noise_points} ç­†")

    return coordinates, labels


def calculate_hotspot_center(
    coordinates: List[Tuple[float, float]],
) -> Tuple[float, float]:
    """è¨ˆç®—ç†±é»ä¸­å¿ƒé»ï¼ˆä½¿ç”¨å¹³å‡å€¼ï¼‰"""
    lats = [coord[0] for coord in coordinates]
    lngs = [coord[1] for coord in coordinates]
    return (sum(lats) / len(lats), sum(lngs) / len(lngs))


def calculate_hotspot_radius(
    coordinates: List[Tuple[float, float]], center: Tuple[float, float]
) -> int:
    """è¨ˆç®—ç†±é»å½±éŸ¿åŠå¾‘ï¼ˆå…¬å°ºï¼‰"""
    if not coordinates:
        return 100  # é è¨­åŠå¾‘

    # è¨ˆç®—æ‰€æœ‰é»åˆ°ä¸­å¿ƒçš„è·é›¢ï¼Œå–æœ€å¤§å€¼
    max_distance = 0
    for coord in coordinates:
        distance = geodesic(center, coord).meters
        max_distance = max(max_distance, distance)

    # åŠ ä¸Šç·©è¡å€ï¼ˆ20%ï¼‰
    radius = int(max_distance * 1.2)
    # é™åˆ¶åœ¨åŠå¾‘ç¯„åœå…§ï¼ˆ50-2000å…¬å°ºï¼‰
    return max(50, min(2000, radius))


def generate_hotspot_records(
    accidents: List[Dict],
    coordinates: np.ndarray,
    labels: np.ndarray,
    min_samples: int,
    analysis_period_start: date,
    analysis_period_end: date,
) -> List[Dict]:
    """
    æ ¹æ“šèšé¡çµæœç”Ÿæˆç†±é»è¨˜éŒ„

    Args:
        accidents: äº‹æ•…è¨˜éŒ„åˆ—è¡¨
        coordinates: åº§æ¨™é™£åˆ—
        labels: èšé¡æ¨™ç±¤
        min_samples: æœ€å°äº‹æ•…æ•¸ï¼ˆéæ¿¾ç”¨ï¼‰
        analysis_period_start: åˆ†ææœŸé–“èµ·å§‹æ—¥æœŸ
        analysis_period_end: åˆ†ææœŸé–“çµæŸæ—¥æœŸ

    Returns:
        ç†±é»è¨˜éŒ„åˆ—è¡¨
    """
    print(f"\nğŸ“ˆ ç”Ÿæˆç†±é»è¨˜éŒ„...")

    hotspots = []
    unique_labels = set(labels)
    if -1 in unique_labels:
        unique_labels.remove(-1)  # ç§»é™¤å™ªéŸ³é»æ¨™ç±¤

    analysis_date = date.today()

    for label in unique_labels:
        # å–å¾—æ­¤èšé¡çš„æ‰€æœ‰äº‹æ•…
        cluster_mask = labels == label
        cluster_indices = np.where(cluster_mask)[0]
        cluster_accidents = [accidents[i] for i in cluster_indices]

        if len(cluster_accidents) < min_samples:
            continue

        # è¨ˆç®—ä¸­å¿ƒé»å’ŒåŠå¾‘
        cluster_coords = [
            (float(acc["latitude"]), float(acc["longitude"]))
            for acc in cluster_accidents
        ]
        center_lat, center_lng = calculate_hotspot_center(cluster_coords)
        radius = calculate_hotspot_radius(cluster_coords, (center_lat, center_lng))

        # çµ±è¨ˆäº‹æ•…æ•¸é‡ï¼ˆæŒ‰ source_type åˆ†é¡ï¼‰
        a1_count = sum(1 for acc in cluster_accidents if acc["source_type"] == "A1")
        a2_count = sum(1 for acc in cluster_accidents if acc["source_type"] == "A2")
        a3_count = sum(1 for acc in cluster_accidents if acc["source_type"] == "A3")

        # æ™‚é–“ç¯„åœ
        occurred_times = [acc["occurred_at"] for acc in cluster_accidents]
        earliest = min(occurred_times)
        latest = max(occurred_times)

        # äº‹æ•… ID åˆ—è¡¨
        accident_ids = [acc["id"] for acc in cluster_accidents]

        hotspot = {
            "id": str(uuid.uuid4()),
            "center_latitude": Decimal(str(center_lat)).quantize(Decimal("0.0000001")),
            "center_longitude": Decimal(str(center_lng)).quantize(Decimal("0.0000001")),
            "radius_meters": radius,
            "total_accidents": len(cluster_accidents),
            "a1_count": a1_count,
            "a2_count": a2_count,
            "a3_count": a3_count,
            "earliest_accident_at": earliest,
            "latest_accident_at": latest,
            "analysis_date": analysis_date,
            "analysis_period_start": analysis_period_start,
            "analysis_period_end": analysis_period_end,
            "accident_ids": json.dumps(accident_ids),
        }

        hotspots.append(hotspot)

    print(f"âœ… ç”Ÿæˆ {len(hotspots)} ç­†ç†±é»è¨˜éŒ„")
    return hotspots


def insert_hotspots(conn, hotspots: List[Dict], clear_existing: bool):
    """
    å°‡ç†±é»è¨˜éŒ„å¯«å…¥ hotspots table

    Args:
        conn: è³‡æ–™åº«é€£ç·š
        hotspots: ç†±é»è¨˜éŒ„åˆ—è¡¨
        clear_existing: æ˜¯å¦æ¸…é™¤ç¾æœ‰è³‡æ–™
    """
    print(f"\nğŸ’¾ å¯«å…¥ç†±é»è³‡æ–™åˆ° hotspots table...")

    with conn.cursor() as cur:
        if clear_existing:
            print("   æ¸…é™¤ç¾æœ‰ç†±é»è³‡æ–™...")
            cur.execute("DELETE FROM hotspots;")
            deleted = cur.rowcount
            print(f"   å·²åˆªé™¤ {deleted} ç­†èˆŠè³‡æ–™")

        # æº–å‚™æ’å…¥è³‡æ–™
        insert_query = """
            INSERT INTO hotspots (
                id,
                center_latitude,
                center_longitude,
                geom,
                radius_meters,
                total_accidents,
                a1_count,
                a2_count,
                a3_count,
                earliest_accident_at,
                latest_accident_at,
                analysis_date,
                analysis_period_start,
                analysis_period_end,
                accident_ids,
                created_at,
                updated_at
            ) VALUES %s
        """

        # æº–å‚™è³‡æ–™ï¼ˆgeom æ¬„ä½æœƒç”± trigger è‡ªå‹•ç”Ÿæˆï¼Œé€™è£¡å‚³ NULLï¼‰
        now = datetime.now(timezone.utc)
        values = [
            (
                h["id"],
                h["center_latitude"],
                h["center_longitude"],
                None,  # geom ç”± trigger è‡ªå‹•ç”Ÿæˆ
                h["radius_meters"],
                h["total_accidents"],
                h["a1_count"],
                h["a2_count"],
                h["a3_count"],
                h["earliest_accident_at"],
                h["latest_accident_at"],
                h["analysis_date"],
                h["analysis_period_start"],
                h["analysis_period_end"],
                h["accident_ids"],
                now,
                now,
            )
            for h in hotspots
        ]

        execute_values(cur, insert_query, values)
        conn.commit()

        print(f"âœ… æˆåŠŸå¯«å…¥ {len(hotspots)} ç­†ç†±é»è¨˜éŒ„")


def print_summary(hotspots: List[Dict]):
    """å°å‡ºåˆ†ææ‘˜è¦"""
    if not hotspots:
        print("\nğŸ“Š åˆ†ææ‘˜è¦ï¼šç„¡ç†±é»ç”Ÿæˆ")
        return

    print(f"\nğŸ“Š åˆ†ææ‘˜è¦ï¼š")
    print(f"   ç¸½ç†±é»æ•¸: {len(hotspots)}")

    total_accidents = sum(h["total_accidents"] for h in hotspots)
    total_a1 = sum(h["a1_count"] for h in hotspots)
    total_a2 = sum(h["a2_count"] for h in hotspots)
    total_a3 = sum(h["a3_count"] for h in hotspots)

    print(f"   æ¶µè“‹äº‹æ•…ç¸½æ•¸: {total_accidents}")
    print(f"   - A1 (æ­»äº¡): {total_a1} ç­†")
    print(f"   - A2 (å—å‚·): {total_a2} ç­†")
    print(f"   - A3 (è²¡æ): {total_a3} ç­†")

    # æ‰¾å‡ºæœ€å±éšªçš„ç†±é»
    most_dangerous = max(hotspots, key=lambda h: h["total_accidents"])
    print(f"\n   æœ€å±éšªç†±é»:")
    print(
        f"   - ä½ç½®: ({most_dangerous['center_latitude']}, {most_dangerous['center_longitude']})"
    )
    print(f"   - äº‹æ•…æ•¸: {most_dangerous['total_accidents']} ç­†")
    print(f"   - åŠå¾‘: {most_dangerous['radius_meters']} å…¬å°º")


def main():
    """ä¸»å‡½æ•¸"""
    args = parse_args()

    print("=" * 60)
    print("ğŸš¦ äº‹æ•…ç†±é» ETL è…³æœ¬")
    print("=" * 60)
    print(f"åˆ†ææœŸé–“: éå» {args.period_days} å¤©")
    print(
        f"DBSCAN åƒæ•¸: epsilon={args.epsilon_meters}m, min_samples={args.min_accidents}"
    )
    print(f"æ¸¬è©¦æ¨¡å¼: {'æ˜¯' if args.dry_run else 'å¦'}")
    print("=" * 60)

    # é€£æ¥è³‡æ–™åº«
    try:
        conn = psycopg2.connect(args.database_url)
        print("âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—: {e}", file=sys.stderr)
        sys.exit(1)

    try:
        # 1. è®€å–äº‹æ•…è³‡æ–™
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=args.period_days)
        accidents = fetch_accidents(conn, cutoff_date)

        if len(accidents) < args.min_accidents:
            print(
                f"âš ï¸  äº‹æ•…æ•¸é‡ä¸è¶³ ({len(accidents)} < {args.min_accidents})ï¼Œç„¡æ³•é€²è¡Œåˆ†æ"
            )
            return

        # 2. åŸ·è¡Œèšé¡åˆ†æ
        coordinates, labels = perform_dbscan_clustering(
            accidents, args.epsilon_meters, args.min_accidents
        )

        # 3. ç”Ÿæˆç†±é»è¨˜éŒ„
        analysis_period_start = cutoff_date.date()
        analysis_period_end = date.today() - timedelta(days=1)

        hotspots = generate_hotspot_records(
            accidents,
            coordinates,
            labels,
            args.min_accidents,
            analysis_period_start,
            analysis_period_end,
        )

        # 4. å¯«å…¥è³‡æ–™åº«
        if not args.dry_run:
            insert_hotspots(conn, hotspots, args.clear_existing)
        else:
            print("\nâš ï¸  æ¸¬è©¦æ¨¡å¼ï¼šä¸å¯«å…¥è³‡æ–™åº«")

        # 5. å°å‡ºæ‘˜è¦
        print_summary(hotspots)

        print("\n" + "=" * 60)
        print("âœ¨ ETL å®Œæˆï¼")
        print("=" * 60)

    except Exception as e:
        conn.rollback()
        print(f"\nâŒ éŒ¯èª¤: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        sys.exit(1)

    finally:
        conn.close()


if __name__ == "__main__":
    main()
